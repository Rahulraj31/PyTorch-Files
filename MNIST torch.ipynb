{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper paramter \n",
    "input_size= 784  #28*28\n",
    "hidden_size=100\n",
    "num_classes=10\n",
    "num_epochs=2\n",
    "batch_size =100\n",
    "learning_rate =0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',train=True,\n",
    "                transform=transforms.ToTensor(), \n",
    "                download=True ) #if not downloaded\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',train=False,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "test_loader =torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#lets see batch \n",
    "examples =iter(train_loader)\n",
    "samples,labels=examples.next()\n",
    "print(samples.shape,labels.shape)   # (batchsize,color channel,size,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0],cmap='gray')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1=nn.Linear(input_size,100)\n",
    "        self.relu =nn.ReLU()\n",
    "        self.l2=nn.Linear(100,50)\n",
    "        self.relu =nn.ReLU()\n",
    "        self.l3 =nn.Linear(50,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.l1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.l2(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1/2,step 100/600,loss=0.3504\n",
      "epoch1/2,step 200/600,loss=0.2944\n",
      "epoch1/2,step 300/600,loss=0.3367\n",
      "epoch1/2,step 400/600,loss=0.2275\n",
      "epoch1/2,step 500/600,loss=0.2735\n",
      "epoch1/2,step 600/600,loss=0.1632\n",
      "epoch2/2,step 100/600,loss=0.1314\n",
      "epoch2/2,step 200/600,loss=0.1696\n",
      "epoch2/2,step 300/600,loss=0.1370\n",
      "epoch2/2,step 400/600,loss=0.1778\n",
      "epoch2/2,step 500/600,loss=0.1332\n",
      "epoch2/2,step 600/600,loss=0.1866\n"
     ]
    }
   ],
   "source": [
    "model=NeuralNet(input_size,100,num_classes)\n",
    "\n",
    "# Loss and optimizer \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#trainind loop\n",
    "\n",
    "n_total_steps = len(train_loader) # for batch\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        # reshaping images i.e. flattening \n",
    "        images=images.reshape(-1,28*28).to(device)\n",
    "        labels=labels.to(device)  \n",
    "    \n",
    "        # forward pass \n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 ==0:\n",
    "            print(f'epoch{epoch+1}/{num_epochs},step {i+1}/{n_total_steps},loss={loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 95.76\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "    \n",
    "    for images,labels in test_loader:\n",
    "        images=images.reshape(-1,28*28).to(device)\n",
    "        labels=labels.to(device)\\\n",
    "        \n",
    "        outputs=model(images)\n",
    "        \n",
    "        #value,index [class label]\n",
    "        _,pred=torch.max(outputs,1)\n",
    "        n_samples+=labels.shape[0]\n",
    "        n_correct+= (pred==labels).sum().item() #to get data inside tensor doesnt affect ans if not used just prettyfy it\n",
    "        \n",
    "print(\"accuracy\",100* n_correct/n_samples)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self,input_size,num_classes):\n",
    "        super(NeuralNet2,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(input_size,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(400,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,200),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Linear(200,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,num_classes),\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.l1(x)\n",
    "        out=self.l2(out)           \n",
    "        out=self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGAR RELU LAYER MAI NA DAAL KE DIRECT LIKHNA HO\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "self.l1 = nn.Sequential(\n",
    "\n",
    "            nn.Linear(input_size,500),\n",
    "            \n",
    "            nn.Linear(500,400)\n",
    "            \n",
    "        )\n",
    "        \n",
    "   \n",
    "def forward(self,x):\n",
    "\n",
    "        out=F.relu(self.l1(x))\n",
    "        \n",
    "        out=F.relu(self.l2(out))    \n",
    "        \n",
    "        out=F.relu(self.l3(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1/2,step 100/600,loss=0.4295\n",
      "epoch1/2,step 200/600,loss=0.4470\n",
      "epoch1/2,step 300/600,loss=0.0929\n",
      "epoch1/2,step 400/600,loss=0.1349\n",
      "epoch1/2,step 500/600,loss=0.1847\n",
      "epoch1/2,step 600/600,loss=0.1216\n",
      "epoch2/2,step 100/600,loss=0.0822\n",
      "epoch2/2,step 200/600,loss=0.2208\n",
      "epoch2/2,step 300/600,loss=0.0544\n",
      "epoch2/2,step 400/600,loss=0.0627\n",
      "epoch2/2,step 500/600,loss=0.0901\n",
      "epoch2/2,step 600/600,loss=0.0254\n"
     ]
    }
   ],
   "source": [
    "model=NeuralNet2(input_size,num_classes)\n",
    "\n",
    "# Loss and optimizer \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#trainind loop\n",
    "\n",
    "n_total_steps = len(train_loader) # for batch\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        # reshaping images i.e. flattening \n",
    "        images=images.reshape(-1,28*28).to(device)\n",
    "        labels=labels.to(device)  \n",
    "    \n",
    "        # forward pass \n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 ==0:\n",
    "            print(f'epoch{epoch+1}/{num_epochs},step {i+1}/{n_total_steps},loss={loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 97.03\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "    \n",
    "    for images,labels in test_loader:\n",
    "        images=images.reshape(-1,28*28).to(device)\n",
    "        labels=labels.to(device)\\\n",
    "        \n",
    "        outputs=model(images)\n",
    "        \n",
    "        #value,index [class label]\n",
    "        _,pred=torch.max(outputs,1)\n",
    "        n_samples+=labels.shape[0]\n",
    "        n_correct+= (pred==labels).sum().item() #to get data inside tensor doesnt affect ans if not used just prettyfy it\n",
    "        \n",
    "print(\"accuracy\",100* n_correct/n_samples)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# -------------------------------------\n",
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Network, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, stride=1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, stride=1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(stride=2, kernel_size=2),  # 30 80\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, stride=1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),   # 15 40\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 15 * 40, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 40)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNN_Network()\n",
    "\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
